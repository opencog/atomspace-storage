                          Persist
                          -------
                Linas Vepstas, Feb-June 2008

A simple-minded implementation of atom persistence into SQL.

STATUS:
Functional but incomplete prototype. Save and restore of atoms, 
and simple truth values, works. Bulk save-and-restore of entire
AtomTable contents works. 

Missing features/ToDo items: 
Add incremental save/restore (i.e. update the SQL contents 
    as AtomTable changes)
Add full support for truth values other than SimpleTruthValue.
Add full support for attention values. (??)
Add full support for Atom deletion.
Add full support for 64-bit systems, including >4Gigs of atoms.

Performance status:
AtomTable load time is more than 3x faster than loading from NMXml;
Also, loading from SQL reduces cogServer memory usage by 3x.
This strongly indicates that the NMXml loader fragments RAM in a 
bad way.


Goals:
------
The goal of this implementation is to:

1) Simplify Linas' day-to-day use of opencog. The problem is that
   loading data, by parsing English sentences, into opencog is slow.
   Every time opencog source is changed, the data needs to be reloaded.
   This makes development tedious.  The hope is that data persistence
   will alleviate data load time.

2) Make sure that the opencog core design is amenable to incremental,
   just-in-time data persistence; that is, the fetching of data 
   as it is needed, and saving it away when its not needed. This
   requires an infrastructure for attention allocation (atoms
   that don't get attention can be saved away to disk, while those
   that are needed are fetched on demand.) This will also require 
   infrastructure for locks, use counts, etc. that are typical
   of multi-threaded programming.  This prototype should lay the 
   foundations for more sophisticated schemes (non-SQL-based) 
   to take its place.

3) Provide a baseline/reference implementation by which other 
   persistence designs can be measured.

Design:
-------
The core design will use a few very simple SQL tables, and some
simple readers and writers to save and restore atoms from an SQL 
database.

Note that the core design does *not* make use of object reflection, 
nor can it store arbitrary kinds of objects. It is very definitely 
hard-wired. Yes, this can be considered to be a short-coming. 
A more general, persistent object framework (for C) can be found 
at http://estron.alioth.debian.org/  However, simplicity, at the
cost of missing flexibility, seems more important.

The current design can save/restore individual atoms, and it can
bulk-save/bulk-restore the entire contents of an AtomTable.
As such, if offers little or no advantage over flat-file storage.
A so-far unrealized goal of the prototype is to implement 
incremental save and restore -- that is, to fetch atoms in a 
"just in time" fashion, and to save away atoms that are not 
needed in RAM (e.g. atoms with low/non-existent attention values).

However, the current AtomTable implementation makes it hard for
the incremental approach to be pursued.  For example, the incoming
set of an atom is expected to be fully populated; and the current
manner in which this is done requires atoms to be fully instantiated.
This makes it somewhat hard to have only a part of a graph in RAM.
However, this is solvable with a bit of elbow grease.

Caveats:
--------
Its not at all clear whether this is a good idea; there are several
major drawbacks to using SQL. These include:
-- Large performance overhead, even if an embedded SQL server were to be used.
-- If/when incremental save/restore is implemented, then there's potentially 
   large churn, if short-lived atoms are constantly created/destroyed.

Getting started
---------------
Create a database called "OpenCog"; for example, in Postgres, at
the Unix command line:

   $ createdb opencog

(you may need to su - postgres; createuser <username> first)
(You may need to edit pg_hba.conf)

Then create the database tables:

   $ cat atom.sql | psql opencog

Next, set up the correct ODBC connection. This version uses iODBC
So edit .odbc.ini in your home directory, and add a stanza similar
to this, adjusting for user and passwd appropriately.  The password
to be supplied is the database password, not the user password.
The database password can be set by doing:

   $ psql -c "ALTER USER linas WITH PASSWORD 'asdf'" -d template1

(Substitute you name and some appropriate passwrd; it does not need to
be your login password, and probably shouldn't be.)

The ODBC stanza is below. You MUST create one of these for EACH
repositry you plan to use! This is to be placed in the ~/.odbc.ini
file in your home directory.

[opencog]
Description    = PostgreSQL
Driver      = PostgreSQL
Trace    = No
TraceFile      =
Database    = opencog
Servername     = localhost
Port     = 5432
Username    = linas
Password    = asdf
ReadOnly    = No
RowVersioning     = No
ShowSystemTables     = Yes
ShowOidColumn     = Yes
FakeOidIndex      = Yes
ConnSettings      =

Next, the postgres default configuration needs to be tweaked for 
performance. Failure to do this will result in disastrous load and
store times.

Edit postgresql.conf (a typical location is 
/etc/postgresql/8.3/main/postgresql.conf) and make the following changes:

   shared_buffers = default was 24MB, change to 250MB
   work_mem = default was 1MB change to 32MB
   fsync = default on  change to off
   synchronous_commit = default on change to off
   wal_buffers = default 64kB change to 512kB
   commit_delay = default 0 change to 10000 (10K) microseconds
   ssl = default true change to false

Restarting the server might lead to errors stating that max shared mem
usage has been exceeded. This can be fixed by:

   vi /etc/sysctl.conf
   kernel.shmmax = 440100100
(save file contents, then)
   sysctl -p /etc/sysctl.conf

Bulk save and restore
---------------------
At last! bulk save of atoms that were previous created is done by
issueing getting to the opencog prompt (netcat localhost 17001) and
issuing the commands:

    opencog> ?
    unknown command >>?<<
            Available commands: data dlopen dlclose exit help load ls scm
            shutdown sql-open sql-close sql-store sql-load
    opencog> sql-open
    sql-open: invalid command syntax
    Usage: sql-open <dbname> <username> <auth>

    opencog> sql-open nlp linas asdf
    Opened "nlp" as user "linas"

    opencog> sql-store
    SQL data store thread started

at this point, a progress indicator will be printed by the opencog 
server, on the servers stdout. It  will say something like:
Stored 236000 atoms.

opencog> sql-close

Next time, you can load the data with:

    opencog> sql-load
    SQL loader thread started

The completion message will be on the server output, for example:

    Finished loading 973300 atoms in total 


Experimental Diary & Results
----------------------------

Store performance
-----------------
This section reviews the performance for storage of data from opencog
to the SQL server (and thence to disk).

First run with a large data set (save of 1564K atoms to the database)
was a disaster.  Huge CPU usage, with 75% of CPU usage occurring in the
kernel block i/o layer, and 12% each for the opencog and postgres times:
   112:00 [md4_raid1] or 4.3 millisecs per atom
   17 minutes for postgres, and opencog, each. or 0.66 millisecs per atom
   1937576 - 1088032 kB = 850MBytes disk use

Experiment: is this due to the bad design for trying to figure whether
"INSERT" or "UPDATE" should be used? A local client-side cache of the 
keys in the DB seems to change little:

   CPU usage for postgres 11:04  and opencog 10:40 and 112:30 for md

So this change drops postgres server and opencog CPU usage
significantly, but the insane kernel CPU usage remains.

The above disaster can be attributed to bad defaults for the postgres
server. In particular, sync to disk, while critical for commercial
database use, is pointless for current use. Also, buffer sizes are much
too small. Edit postgresql.conf and make the following changes:

   shared_buffers = default was 24MB, change to 384MB
   work_mem = default was 1MB change to 32MB
   fsync = default on  change to off
   synchronous_commit = default on change to off
   wal_buffers = default 64kB change to 512kB
   commit_delay = default 0 change to 10000 (10K) microseconds
   ssl = default true change to false

Restarting the server might lead to errors stating that max shared mem
usage has been exceeded. This can be fixed by:

   vi /etc/sysctl.conf
   kernel.shmmax = 440100100
(save file contents, then)
   sysctl -p /etc/sysctl.conf

After tuning, save of data to empty DB gives result:
   cogserver = 10:45 mins = 0.41  millisecs/atom (2.42K atoms/sec)
   postgres  =  7:32 mins = 0.29  millisecs/atom (2.65K atoms/sec)
   md        =  0:42 mins = 0.026 millisecs/atom (37K atoms/sec)

Try again, dropping the indexes on the atom and edge tables. Then,
after loading all atoms, rebuild the index. This time, get
   cogserver = 5:49 mins = 0.227 millisecs/atom (4.40K atoms/sec)
   postgres  = 4:50 mins = 0.189 millisecs/atom (5.30K atoms/sec)

Try again, this time with inline outgoing sets. This improves 
performance even further:

   cogserver = 2:54 mm:ss = 0.113 millisecs/atom (8.83K atoms/sec)
   postgres  = 2:22 mm:ss = 0.092 millisecs/atom (10.82K atoms/sec)

Try again, compiled with -O3, storing to an empty table, with
no indexes on it (and with inline outgoing sets):
   cogserver = 2:40 mm:ss
   postgres  = 2:16 mm:ss

Try again, compiled with -O3, storing to empty table, while holding 
the index on tables (and with inline outgoing sets).

   cogserver = 2:51 mm:ss
   postgres  = 2:06 mm:ss

Appearently, the problem with the indexes has to do with holding them
for the edge table; when there's no edge table, then there's no index 
issue!?

Try again, compiled with -O3, saving to (updating) a *full* database.
(i.e. database already has the data, so we are doing UPDATE not INSERT)
   cogserver = 2:19 mm:ss
   postgres  = 4:35 mm:ss

Try again, using unixodbc instead of iodbc, to empty table, withOUT
index tables (and -O3 and inlined outgoing):
   cogserver = 2:36 mm:ss
   postgres  = 2:13 mm:ss

It appears that unixodbc is essentially identical to iodbc performance

begin; commit;  
use analyze;
use prepare;

XML loading performance
-----------------------
Loading the dataset from XML files takes:

   cogserver 2:34 mm:ss when compiled without optimization
   cogserver 1:19 mm:ss when compiled with -O3 optimization

Loading performance
-------------------
Loading performance. Database contains 1564K Atoms, and 2413K edges.
CPU usage:
2:08 postgres = 82 microsecs/atom (12.2K atoms/sec)
similar to for opencog, but then AtomTable needs to reconcile, which
takes an additional 8:30 minutes to attach incoming handles!!

Conclude: database loading would be much faster if we loaded all of
the atoms first, then all of the lowest-height links, etc.  This assumes
that opencog is strictly hierarchically structured. (no "crazy loops")

After implementing height-structured restore, get following, loading
from a "hot" postgres instance (had not been stopped since previous
use, i.e. data should have been hot in RAM):
  cogserver = 2:36 mm:ss = 0.101 millisecs/atom (9.85K atoms/sec)
  postgres  = 1:59 mm:ss = 0.077 millisecs/atom (12.91K atoms/sec)

The dataset had 357162 Nodes, 1206544 Links at height 1

After a cold start, have

  cogserver = 2:32 mm:ss
  postgres  = 1:55 mm:ss

Appears that there is no performance degradation for cold-starts.

Note also: cogServer CPU usage is *identical* to its CPU usage when
loading XML! Hurrah! Also, see below: RAM usage is significantly 
reduced; apparently, the reading of XML results in very bad memory
fragmentation.

Implement inline edges, instead of storing eges in an outboard table.

  cogserver = 41 seconds = 26.7 microsecs/atom (37.5K atoms/sec)
  postgres  =  7 seconds =  4.56 microsecs/atom (219K atoms/sec)

Turn on -O3 optimization during compile ... all previous figures
were without *any* optimization. Now get 

  cogserver = 24 seconds = 15.6 microsecs/atom (64.0K atoms/sec)
  postgres  = 11 seconds

Much much better!

i10.78
23.15


Evidence of sever memory fragmentation
--------------------------------------
loading 1536K Atoms from XML results in cogserver using 1210 MBytes ram.
loading 1536K Atoms from SQL results in cogserver using 633 MBytes ram.
loading 1536K Atoms from SQL (with new record managemnt) results in 
cogserver using 442 MBytes ram.

Almost one-third the ram !!

The fragmentation occurs when the XML is piped to cogserver much faster
than the cogserver can process it; the fragmentation is between data
buffered from the sockets, and the cogserver itself.  The fragementation
does *not* occur when the data is piped in more slowly,i.e. at the same 
rate that opencog processes it.


TODO
----
-- Store more complex truth value types

-- Store attention values.

-- Loading from SQL uses half as much ram as loading from XML, which
   suggests the XML loader has a memory leak!?  Need to run valgrind on
   it.

